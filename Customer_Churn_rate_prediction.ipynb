{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem: Churn Rate Customer**\n",
        "\n",
        "## **Introduction:**\n",
        "\n",
        "To the untrained eye, customer behavior is difficult to predict. After all, they are humans with erratic whims and desires. However, to a machine that can compute thousands of things a second, trends and patterns are increasingly obvious. Businesses aim to engage with customers in a way that they return to the store repeatedly, generating revenue each time. However, it can be challenging to determine which customers are likely to return, and which customers have lost interest in the goods or services being provided.\n",
        "\n",
        "Here we introduce the concept of Customer Churn: A customer is considered churn-ing if they are actively returning to the store. Whereas a churn-ed customer is one that is no longer coming back for more.\n",
        "\n",
        "**Customer Churn Risk** is the probability that a customer will disengage with the business."
      ],
      "metadata": {
        "id": "f5SAlS-4bNlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Churn rate** is a marketing metric that describes the number of customers who leave a business over a specific time period. Every user is assigned a prediction value that estimates their state of churn at any given time. This value is based on:\n",
        "\n",
        "User demographic information Browsing behavior Historical purchase data among other information It factors in our unique and proprietary predictions of how long a user will remain a customer. This score is updated every day for all users who have a minimum of one conversion. The values assigned are between 1 and 5"
      ],
      "metadata": {
        "id": "5YGj7oP0RE2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Customers Chur Prediction with SVM using Scikit-Learn**\n",
        "\n",
        "**Support Vector Machine (SVM)** is unique among the supervised machine learning algorithms in the sense that it focuses on training data points along the separating hyper planes."
      ],
      "metadata": {
        "id": "3R9u8rYCbWRi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rijZs3cGPQXD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import svm, datasets\n",
        "import sklearn.model_selection as model_selection\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import scipy.stats as stats\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import recall_score, accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from imblearn.combine import SMOTEENN\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from collections import Counter\n",
        "\n",
        "# ignore warning\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.ticker as mtick # for showing percentage in it"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/churn_rate_prediction.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4bYxRM8vPXcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "C48Q2etQPg8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get overview of the data**"
      ],
      "metadata": {
        "id": "67BAObazPjy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_overview(df, message):\n",
        "  print(f'{message}:')\n",
        "  print('Number of rows: ', df.shape[0])\n",
        "  print('Number of features: ', df.shape[1])\n",
        "  print('Data Feature')\n",
        "  print(df.columns.tolist())\n",
        "  print('Missing Values: ', df.isnull().sum().values.sum())\n",
        "  print('Unique values:')\n",
        "  print(df.nunique())\n",
        "\n",
        "data_overview(df, 'Over view of the dataset')"
      ],
      "metadata": {
        "id": "ZF3ZU-3jPmIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"churn_risk_score\"] = df[\"churn_risk_score\"].replace(-1,1)"
      ],
      "metadata": {
        "id": "_GorZq8sT8NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "QQNpaZxBV5Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"churn_risk_score\"].value_counts()"
      ],
      "metadata": {
        "id": "2SHHvqhYPuyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From analysis the dataset, we notice that some of the independent features are in numerical and most of feature are in categorical feature. Seperate into numerical and categorical data for EDA parts**"
      ],
      "metadata": {
        "id": "AWxbu_TbP05U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Separate into Numerical and Categorical feature**"
      ],
      "metadata": {
        "id": "HbC0xRWmP2H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# numerical feature\n",
        "numerical_feature = {feature for feature in df.columns if df[feature].dtypes != 'O'}\n",
        "print(f'Count of Numerical feature: {len(numerical_feature)}')\n",
        "print(f'Numerical feature are:\\n {numerical_feature}')"
      ],
      "metadata": {
        "id": "k_PgG86KP697"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical feature\n",
        "categorical_feature = {feature for feature in df.columns if df[feature].dtypes == 'O'}\n",
        "print(f'Count of Categorical feature: {len(categorical_feature)}')\n",
        "print(f'Categorical feature are:\\n {categorical_feature}')"
      ],
      "metadata": {
        "id": "2FrAwgjiP7zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EDA**"
      ],
      "metadata": {
        "id": "NzxsXiKRQBkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting with target feature\n",
        "sns.countplot(data=df, x='churn_risk_score')\n",
        "plt.title('Count of Churn')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7-1KsYQGVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Plotting numerical feature with probability distribution and checking outlier\n",
        "for feature in numerical_feature:\n",
        "    if feature != 'past_complaint':\n",
        "        plt.figure(figsize=(15,7))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        sns.histplot(data=df, x=feature, bins=30, kde=True)\n",
        "        plt.title('Histogram')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        stats.probplot(df[feature], dist=\"norm\", plot=plt)\n",
        "        plt.ylabel('RM quantiles')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        sns.boxplot(x=df[feature])\n",
        "        plt.title('Boxplot')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0me3y0ESTJpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "49ZTzMhTTmK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df.drop(columns='past_complaint'),hue='churn_risk_score', kind='scatter')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t4imxY81TmaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Univariate Analysis:**"
      ],
      "metadata": {
        "id": "O6nNBFpGX1oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, feature in enumerate(categorical_feature):\n",
        "    if feature != 'past_complaint':\n",
        "        if feature != 'customer_id':\n",
        "            plt.figure(i)\n",
        "            plt.figure(figsize=(12,6))\n",
        "            sns.countplot(data=df, x=feature, hue='churn_risk_score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4Qxw_Bklb0rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Cleaning**"
      ],
      "metadata": {
        "id": "ZFbvS32yYB5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "X6QV0M77YDPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "fBYEqJaFYEuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace NaN values with mean value\n",
        "df.points_in_wallet = df.points_in_wallet.fillna(df.points_in_wallet.median())"
      ],
      "metadata": {
        "id": "8omCuY09YLgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.points_in_wallet.hist()"
      ],
      "metadata": {
        "id": "SHpz7yLJYR4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding**"
      ],
      "metadata": {
        "id": "pz6gIi0AYh5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(categorical_feature)"
      ],
      "metadata": {
        "id": "tFZtH7MpYiEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "for feature in categorical_feature:\n",
        "    df[feature] = encoder.fit_transform(df[feature])"
      ],
      "metadata": {
        "id": "EHXvkkOZYmLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "R1P8vXAQYq-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['customer_id'], inplace=True)"
      ],
      "metadata": {
        "id": "x5K-v3_AYt4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Correlation of \"Churn\" with other variables:\n",
        "plt.figure(figsize=(15,8))\n",
        "df.corr()['churn_risk_score'].sort_values(ascending = False).plot(kind='bar')"
      ],
      "metadata": {
        "id": "z2Mr199bY1Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the correlation between the independent and dependent feature\n",
        "plt.figure(figsize=(20, 9))\n",
        "sns.heatmap(df.corr(), annot=True)"
      ],
      "metadata": {
        "id": "ZrTrGJt0Y_aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting the dataset into train and test**"
      ],
      "metadata": {
        "id": "KUP8d0dcZE55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting dataset into dependent and independent feature\n",
        "X = df.drop(columns='churn_risk_score')\n",
        "y = df['churn_risk_score']"
      ],
      "metadata": {
        "id": "e0H7QQgPZHIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "I_fjDLi2ZM2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection:\n",
        "Selection 10 feature that have high correllation"
      ],
      "metadata": {
        "id": "C83bcUSHZPzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selects the feature which has more correlation\n",
        "selection = SelectKBest()  # k=10 default\n",
        "X = selection.fit_transform(X,y)"
      ],
      "metadata": {
        "id": "2jOSPPI3ZVyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this will shows which feature are taken denote as True other are removed like false\n",
        "selection.get_support()"
      ],
      "metadata": {
        "id": "Rvfh-7y7ZYz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting for train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "sCgG4zYSZhbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "2E_3u2LmZjcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "yI0dYJf9Zj54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# its an imbalance dataset\n",
        "y.value_counts()"
      ],
      "metadata": {
        "id": "jkcxwHmoZlIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Apply into machine learning algorithm:**"
      ],
      "metadata": {
        "id": "8aS7cPLCZpAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Log_reg = LogisticRegression(C=150, max_iter=150)\n",
        "Log_reg.fit(X_train, y_train)\n",
        "log_pred = Log_reg.predict(X_test)\n",
        "\n",
        "print(f'Accuracy score : {accuracy_score(log_pred, y_test)}')\n",
        "print(f'Confusion matrix :\\n {confusion_matrix(log_pred, y_test)}')\n",
        "print(f'Classification report :\\n {classification_report(log_pred, y_test)}')"
      ],
      "metadata": {
        "id": "eRvsYzeUZqdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random forest classifier\n",
        "Rfc = RandomForestClassifier(n_estimators=120,criterion='gini', max_depth=15, min_samples_leaf=10, min_samples_split=5)\n",
        "Rfc.fit(X_train, y_train)\n",
        "rfc_pred = Rfc.predict(X_test)\n",
        "\n",
        "print(f'Accuracy score : {accuracy_score(rfc_pred, y_test)}')\n",
        "print(f'Confusion matrix :\\n {confusion_matrix(rfc_pred, y_test)}')\n",
        "print(f'Classification report :\\n {classification_report(rfc_pred, y_test)}')"
      ],
      "metadata": {
        "id": "XfdYSMbTZ6VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decisionTree Classifier\n",
        "Dtc = DecisionTreeClassifier(criterion='gini', splitter='random', min_samples_leaf=15)\n",
        "Dtc.fit(X_train, y_train)\n",
        "dtc_pred = Dtc.predict(X_test)\n",
        "\n",
        "print(f'Accuracy score : {accuracy_score(dtc_pred, y_test)}')\n",
        "print(f'Confusion matrix :\\n {confusion_matrix(dtc_pred, y_test)}')\n",
        "print(f'Classification report :\\n {classification_report(dtc_pred, y_test)}')"
      ],
      "metadata": {
        "id": "5JQD6JIdZ-U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we compare to the imbalance dataset our model are perform like okay not a better model to build for end to end project. So we need to over smpling data for reducing the TN, FN and increase the FP and TP for model building**"
      ],
      "metadata": {
        "id": "ytIbrIPkaE4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using SMOTEENN for imbalance dataset:**\n",
        "\n",
        "Over-sampling using SMOTE and cleaning using ENN. Combine over- and under-sampling using SMOTE and Edited Nearest Neighbours"
      ],
      "metadata": {
        "id": "jwGSn-gwaKBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "st=SMOTEENN()\n",
        "X_train_st,y_train_st = st.fit_resample(X_train, y_train)\n",
        "print(\"The number of classes before fit {}\".format(Counter(y_train)))\n",
        "print(\"The number of classes after fit {}\".format(Counter(y_train_st)))"
      ],
      "metadata": {
        "id": "HFbEB65TaRPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the over sampling dataset\n",
        "X_train_sap, X_test_sap, y_train_sap, y_test_sap = train_test_split(X_train_st, y_train_st, test_size=0.2)"
      ],
      "metadata": {
        "id": "pwM7ySc2aURh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decisionTree Classifier\n",
        "Dtc_sampling = DecisionTreeClassifier(criterion = \"gini\",random_state = 100,max_depth=7, min_samples_leaf=15)\n",
        "Dtc_sampling.fit(X_train_sap, y_train_sap)\n",
        "dtc_sampling_pred = Dtc_sampling.predict(X_test_sap)\n",
        "\n",
        "print(f'Accuracy score : {accuracy_score(dtc_sampling_pred, y_test_sap)}')\n",
        "print(f'Confusion matrix :\\n {confusion_matrix(dtc_sampling_pred, y_test_sap)}')\n",
        "print(f'Classification report :\\n {classification_report(dtc_sampling_pred, y_test_sap)}')"
      ],
      "metadata": {
        "id": "8Y_UhIm8aVMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random forest classifier\n",
        "Rfc_sampling = RandomForestClassifier(n_estimators=150,criterion='gini', max_depth=15, min_samples_leaf=10, min_samples_split=6)\n",
        "Rfc_sampling.fit(X_train_sap, y_train_sap)\n",
        "rfc_sampling_pred = Rfc_sampling.predict(X_test_sap)\n",
        "\n",
        "print(f'Accuracy score : {accuracy_score(rfc_sampling_pred, y_test_sap)}')\n",
        "print(f'Confusion matrix :\\n {confusion_matrix(rfc_sampling_pred, y_test_sap)}')\n",
        "print(f'Classification report :\\n {classification_report(rfc_sampling_pred, y_test_sap)}')"
      ],
      "metadata": {
        "id": "JwzuU4sfdZ0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logistic regression\n",
        "Log_reg_sampling = LogisticRegression(C=10, max_iter=150)\n",
        "Log_reg_sampling.fit(X_train_sap, y_train_sap)\n",
        "Log_sampling_pred = Log_reg_sampling.predict(X_test_sap)\n",
        "\n",
        "print(f'Accuracy score : {accuracy_score(Log_sampling_pred, y_test_sap)}')\n",
        "print(f'Confusion matrix :\\n {confusion_matrix(Log_sampling_pred, y_test_sap)}')\n",
        "print(f'Classification report :\\n {classification_report(Log_sampling_pred, y_test_sap)}')"
      ],
      "metadata": {
        "id": "6kYdMNvadg4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GradientBoostingClassifier\n",
        "gbc = GradientBoostingClassifier()\n",
        "gbc.fit(X_train_sap, y_train_sap)\n",
        "pred = gbc.predict(X_test_sap)\n",
        "\n",
        "print(f'Accuracy score : {accuracy_score(pred, y_test_sap)}')\n",
        "print(f'Confusion matrix :\\n {confusion_matrix(pred, y_test_sap)}')\n",
        "print(f'Classification report :\\n {classification_report(pred, y_test_sap)}')"
      ],
      "metadata": {
        "id": "9iyaCKtrdmNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After using SMOOTEENN methods to perform over sampling and down sampling with edited nearest neighbours. From this results we get bet accuraccy and TP FP ratio also increases in GradientBoostClassifier so perform HyperParameter Tunning for this model only**"
      ],
      "metadata": {
        "id": "_qTaoUvGafR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_estimators':[100, 150, 200, 250, 300],\n",
        "             'criterion': ['friedman_mse', 'squared_error', 'mse', 'mae'],\n",
        "             'min_samples_split': [2,3,4,5,6,7,8,9,10],\n",
        "             'min_samples_leaf': [1,3,5,7,9,11,13,15],'max_leaf_nodes': [3,6,8,9,12,15,18,24],\n",
        "              'max_depth': [3,5,7,9,11,13,15,17,19],\n",
        "              'learning_rate': [0.05, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "              'loss': ['deviance', 'exponential']\n",
        "              }"
      ],
      "metadata": {
        "id": "rHexjiIIanzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbc_optm = RandomizedSearchCV(estimator=gbc, param_distributions=param_grid,n_iter=100, verbose=3)\n",
        "gbc_optm.fit(X_train_sap, y_train_sap)"
      ],
      "metadata": {
        "id": "Shv1dJy2asZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbc_optm.best_estimator_"
      ],
      "metadata": {
        "id": "kX_skaMAerUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GradientBoostingClassifier\n",
        "gbc_tunning = GradientBoostingClassifier(criterion='squared_error', learning_rate=0.3,\n",
        "                           max_depth=19, max_leaf_nodes=24, min_samples_leaf=9,\n",
        "                           min_samples_split=7, n_estimators=150)\n",
        "gbc_tunning.fit(X_train_sap, y_train_sap)\n",
        "pred = gbc_tunning.predict(X_test_sap)\n",
        "\n",
        "print(f'Accuracy score : {accuracy_score(pred, y_test_sap)}')\n",
        "print(f'Confusion matrix :\\n {confusion_matrix(pred, y_test_sap)}')\n",
        "print(f'Classification report :\\n {classification_report(pred, y_test_sap)}')"
      ],
      "metadata": {
        "id": "OXDmUfB0esIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After Oversampling the dataset our model performs is pretty good. From our 4 model GradientBoostClassifier performs better than all.**"
      ],
      "metadata": {
        "id": "-xRG4CsiezdW"
      }
    }
  ]
}